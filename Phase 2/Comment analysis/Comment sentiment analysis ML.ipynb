{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from hazm import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "import re\n",
    "import emoji\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_path = \"../stopwords\"\n",
    "STOPWORDS = set([\n",
    "    \"از\", \"به\", \"در\", \"با\", \"که\", \"را\", \"تا\", \"و\", \"یا\", \"اما\", \"اگر\", \"برای\", \"بر\", \n",
    "    \"این\", \"آن\", \"یک\", \"هر\", \"هم\", \"همه\", \"چند\", \"چنین\", \"دیگر\", \"چون\", \"مثل\", \n",
    "    \"مانند\", \"چرا\", \"زیرا\", \"ولی\", \"آیا\", \"اگرچه\", \"لذا\", \"نیز\", \"باید\", \"می\", \n",
    "    \"باشد\", \"است\", \"بود\", \"هست\", \"شد\", \"شو\", \"باش\", \"کرد\", \"کن\", \"کند\", \"کرده\", \n",
    "    \"شده\", \"می‌شود\", \"خواهد\", \"خواهند\", \"خواهی\", \"خواهیم\", \"توان\", \"تواند\", \n",
    "    \"توانند\", \"توانست\", \"توانسته\", \"بوده\", \"نبود\", \"نباشد\", \"نیست\", \"نیستند\", \n",
    "    \"بودند\", \"باشند\", \"هستند\", \"دارم\", \"داری\", \"دارد\", \"دارند\", \"داریم\", \"داشت\", \n",
    "    \"داشتند\", \"داشته\", \"داشتم\", \"ندارم\", \"ندارد\", \"ندارند\", \"نداریم\", \"نداشت\", \n",
    "    \"نداشتند\", \"نداشته\", \"ای\", \"ایم\", \"اید\", \"اند\", \"ام\", \"ت\", \"ها\", \"های\", \"هایی\", \n",
    "    \"شان\", \"ش\", \"مان\", \"تان\", \"اینها\", \"آنها\", \"چیز\", \"چیزی\", \"چرا\", \"چه\", \"که\", \n",
    "    \"کدام\", \"چگونه\", \"چقدر\", \"چراکه\", \"آنان\", \"او\", \"آن\", \"ایشان\", \"ما\", \"شما\", \n",
    "    \"آنچه\", \"آنجا\", \"اینجا\", \"اینجاست\", \"آنجاست\", \"همان\", \"خود\", \"همه‌اش\", \n",
    "    \"هیچ\", \"هیچ‌کدام\", \"هرگز\", \"هیچگاه\", \"حالا\", \"اکنون\", \"دیروز\", \"امروز\", \n",
    "    \"فردا\", \"شب\", \"روز\", \"بعد\", \"قبل\", \"ساعت\", \"وقت\", \"زمان\", \"چندین\", \"بار\", \n",
    "    \"کم\", \"بیشتر\", \"کمتر\", \"حتی\", \"فقط\", \"تنها\", \"بالا\", \"پایین\", \"روی\", \"زیر\", \n",
    "    \"جلو\", \"پشت\", \"نزدیک\", \"دور\", \"وسط\", \"بیرون\", \"درون\", \"داخل\", \"کنار\", \n",
    "    \"اینجا\", \"آنجا\", \"هیچ‌جا\", \"هرجا\", \"هرکجا\", \"جا\", \"مکان\", \"محل\", \"چپ\", \"راست\", \n",
    "    \"بعدا\", \"سپس\", \"آنگاه\", \"دیگر\", \"چیزهای\", \"یعنی\", \"خب\", \"آره\", \"نه\", \"باشه\", \n",
    "    \"آها\", \"بله\", \"نمیدانم\", \"کسی\", \"دیگری\", \"هیچ‌کسی\", \"چیزها\"\n",
    "])\n",
    "# for filename in os.listdir(folder_path):\n",
    "#     file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "#     if os.path.isfile(file_path) and filename.endswith(\".txt\"):  \n",
    "#         with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "#             words = file.read().split()\n",
    "#             STOPWORDS.update(words)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('../data/BaSalam.reviews.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = reviews[(reviews['description'].notna())][['description', 'star']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_sticker(token):\n",
    "    # بررسی فرمت فایل\n",
    "    if re.match(r'.*\\.(webp|png|gif|jpg)$', token):\n",
    "        return True\n",
    "    # بررسی ایموجی\n",
    "    if emoji.is_emoji(token):\n",
    "        return True\n",
    "    # بررسی لینک\n",
    "    if re.match(r'https?://[^\\s]+', token):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "stemmer = Stemmer()\n",
    "\n",
    "\n",
    "def preprocessing(comment):\n",
    "    # حذف ایموجی‌ها\n",
    "    comment = emoji.replace_emoji(comment, replace=\"\")\n",
    "    # حذف لینک‌ها\n",
    "    comment = re.sub(r'https?://\\S+|www\\.\\S+', '', comment)\n",
    "    # حذف علامت‌های نگارشی\n",
    "    comment = re.sub(r'[^\\w\\s]', '', comment)\n",
    "    # حذف اعداد\n",
    "    comment = re.sub(r'\\d+', '', comment)\n",
    "    text =  comment\n",
    "    normalized = normalizer.normalize(text)\n",
    "    tokens = word_tokenize(normalized)\n",
    "    filtered = []\n",
    "    for token in tokens:\n",
    "        token = str(token)\n",
    "        token = token.lower()\n",
    "        token = re.sub(r'[\\u200c\\u200b\\u200d]', ' ', token)\n",
    "        if not token in STOPWORDS and not token.isdigit() and not is_sticker(token):\n",
    "            filtered.append(token)\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "# def preprocessing(text):\n",
    "#     punc_removed = text.translate(str.maketrans('', '', string.punctuation))\n",
    "#     normalized = normalizer.normalize(punc_removed)\n",
    "#     stemmed = stemmer.stem(normalized)\n",
    "#     tokens = word_tokenize(stemmed)\n",
    "#     filtered = []\n",
    "#     for token in tokens:\n",
    "#         token = str(token)\n",
    "#         token = token.lower()\n",
    "#         if not token in stopwords_list() and not token.isdigit():\n",
    "#             filtered.append(token)\n",
    "#     return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['satisfaction'] = df['star'].apply(lambda x: 1 if x > 3 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, to_much = train_test_split(df, test_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = dict(df_train['satisfaction'].value_counts())\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(cnt.keys())\n",
    "sizes = list(cnt.values())\n",
    "fig = px.histogram(x=labels, y=sizes)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "groupby_rate = df_train.groupby('star')['star'].count()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=list(sorted(groupby_rate.index)),\n",
    "    y=groupby_rate.tolist(),\n",
    "    text=groupby_rate.tolist(),\n",
    "    textposition='auto'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text='Distribution of star within comments',\n",
    "    xaxis_title_text='Rate',\n",
    "    yaxis_title_text='Frequency',\n",
    "    bargap=0.2,\n",
    "    bargroupgap=0.2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['len_comment_by_word'] = df_train['description'].apply(lambda x: len(word_tokenize(x)))\n",
    "min_max_len = df_train[\"len_comment_by_word\"].min(), df_train[\"len_comment_by_word\"].max()\n",
    "print(f'Min: {min_max_len[0]} \\tMax: {min_max_len[1]}')\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "groupby_rate = df_train.groupby('len_comment_by_word')['len_comment_by_word'].count()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=list(sorted(groupby_rate.index)),\n",
    "    y=groupby_rate.tolist(),\n",
    "    text=groupby_rate.tolist(),\n",
    "    textposition='auto'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text='Distribution of comment words within comments',\n",
    "    xaxis_title_text='Rate',\n",
    "    yaxis_title_text='Frequency',\n",
    "    bargap=0.2,\n",
    "    bargroupgap=0.2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['cleaned_comment'] = df_train['description'].apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['len_comment_after_preprocess_by_word'] = df_train['cleaned_comment'].apply(lambda x: len(word_tokenize(x)))\n",
    "min_max_len = df_train[\"len_comment_after_preprocess_by_word\"].min(), df_train[\"len_comment_after_preprocess_by_word\"].max()\n",
    "print(f'Min: {min_max_len[0]} \\tMax: {min_max_len[1]}')\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "groupby_rate = df_train.groupby('len_comment_after_preprocess_by_word')['len_comment_after_preprocess_by_word'].count()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=list(sorted(groupby_rate.index)),\n",
    "    y=groupby_rate.tolist(),\n",
    "    text=groupby_rate.tolist(),\n",
    "    textposition='auto'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text='Distribution of comment words after preprocess within comments',\n",
    "    xaxis_title_text='Rate',\n",
    "    yaxis_title_text='Frequency',\n",
    "    bargap=0.2,\n",
    "    bargroupgap=0.2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(df_train[df_train['cleaned_comment'].apply(lambda x: len(word_tokenize(x)) > 1)==False].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "groupby_rate = df_train.groupby('len_comment_after_preprocess_by_word')['len_comment_after_preprocess_by_word'].count()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=list(sorted(groupby_rate.index)),\n",
    "    y=groupby_rate.tolist(),\n",
    "    text=groupby_rate.tolist(),\n",
    "    textposition='auto'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text='Distribution of comment words after preprocess within comments',\n",
    "    xaxis_title_text='Rate',\n",
    "    yaxis_title_text='Frequency',\n",
    "    bargap=0.2,\n",
    "    bargroupgap=0.2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_train.reset_index()[['cleaned_comment', 'satisfaction']]\n",
    "data.columns = ['comment', 'label']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "groupby_label = data.groupby('label')['label'].count()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=list(sorted(groupby_label.index)),\n",
    "    y=groupby_label.tolist(),\n",
    "    text=groupby_label.tolist(),\n",
    "    textposition='auto'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text='Distribution of label within comments [DATA]',\n",
    "    xaxis_title_text='Label',\n",
    "    yaxis_title_text='Frequency',\n",
    "    bargap=0.2,\n",
    "    bargroupgap=0.2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_data = data[data['label'] == 0]\n",
    "positive_data = data[data['label'] == 1]\n",
    "\n",
    "cutting_point = min(len(negative_data), len(positive_data))\n",
    "\n",
    "if cutting_point <= len(negative_data):\n",
    "    negative_data = negative_data.sample(n=cutting_point).reset_index(drop=True)\n",
    "\n",
    "if cutting_point <= len(positive_data):\n",
    "    positive_data = positive_data.sample(n=cutting_point).reset_index(drop=True)\n",
    "\n",
    "new_data = pd.concat([negative_data, positive_data])\n",
    "new_data = new_data.sample(frac=1).reset_index(drop=True)\n",
    "new_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "groupby_label = new_data.groupby('label')['label'].count()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=list(sorted(groupby_label.index)),\n",
    "    y=groupby_label.tolist(),\n",
    "    text=groupby_label.tolist(),\n",
    "    textposition='auto'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text='Distribution of label within comments [NEW DATA]',\n",
    "    xaxis_title_text='Label',\n",
    "    yaxis_title_text='Frequency',\n",
    "    bargap=0.2,\n",
    "    bargroupgap=0.2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(new_data['comment'], new_data['label'], test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes = Pipeline([('vect', CountVectorizer(tokenizer=tokenize,\n",
    "                                              analyzer='word', ngram_range=(1, 2), min_df=1, lowercase=False)),\n",
    "                     ('tfidf', TfidfTransformer(sublinear_tf=True)),\n",
    "                     ('clf', MultinomialNB())])\n",
    "naive_bayes = naive_bayes.fit(X_train, y_train)\n",
    "naive_score = naive_bayes.score(X_test, y_test)\n",
    "print('Naive Bayes Model: ', naive_score)\n",
    "predict_nb = naive_bayes.predict(X_test)\n",
    "print(\"f1 score : \",f1_score(predict_nb, y_test))\n",
    "print(classification_report(predict_nb, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_count = y_test.value_counts().to_dict()\n",
    "\n",
    "prior_probability = {key: value / len(data) for key, value in class_count.items()}\n",
    "\n",
    "def token_counter(texts):\n",
    "    count_dict = {}\n",
    "    for text in tqdm(texts):\n",
    "        preprocessed = preprocessing(text)\n",
    "        for token in preprocessed:\n",
    "            if token in count_dict:\n",
    "                count_dict[token] += 1\n",
    "            else:\n",
    "                count_dict[token] = 1\n",
    "    return count_dict\n",
    "\n",
    "negative_class_count = token_counter(new_data[new_data['label'] == 0]['comment'])\n",
    "print(f'Negative class - Vocab size: {len(negative_class_count)}, Total count: {sum(negative_class_count.values())}')\n",
    "\n",
    "positive_class_count = token_counter(new_data[new_data['label'] == 1]['comment'])\n",
    "print(f'Positive class - Vocab size: {len(positive_class_count)}, Total count: {sum(positive_class_count.values())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_based_count = [negative_class_count, positive_class_count]\n",
    "vocab_size = len(set(list(negative_class_count.keys()) + list(positive_class_count.keys())))\n",
    "total_count = [sum(negative_class_count.values()), sum(positive_class_count.values())] \n",
    "\n",
    "\n",
    "def compute_probability(text, cls):\n",
    "    total_probability = 1.0\n",
    "    preprocessed = preprocessing(text)\n",
    "    for token in preprocessed:\n",
    "        try:\n",
    "            word_count = class_based_count[cls][token]\n",
    "        except:\n",
    "            word_count = 0\n",
    "        word_prob = (word_count + 1) / (total_count[cls] + vocab_size + 1)\n",
    "        total_probability = total_probability * word_prob\n",
    "    total_probability = total_probability * prior_probability[cls]\n",
    "    return total_probability\n",
    "\n",
    "\n",
    "def predict(test):\n",
    "    predictions = []\n",
    "    for text in test:\n",
    "        neg_prob = compute_probability(text, 0)\n",
    "        pos_prob = compute_probability(text, 1)\n",
    "        if neg_prob > pos_prob:\n",
    "            predictions.append(0)\n",
    "        else:\n",
    "            predictions.append(1)\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = predict(X_test)\n",
    "print(accuracy_score(y_test, test_predictions))\n",
    "print(classification_report(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Support Vector Machine Model\n",
    "svm = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize, analyzer='word', ngram_range=(1, 2), min_df=1, lowercase=False)),\n",
    "    ('tfidf', TfidfTransformer(sublinear_tf=True)),\n",
    "    ('clf-svm', LinearSVC(loss='hinge', penalty='l2', max_iter=5))\n",
    "                ])\n",
    "\n",
    "svm = svm.fit(X_train, y_train)\n",
    "linear_svc_score = svm.score(X_test, y_test)\n",
    "print('Linear SVC Model: ', linear_svc_score)\n",
    "predict_svm = svm.predict(X_test)\n",
    "print( \"f1 score : \", f1_score(predict_svm, y_test))\n",
    "print(classification_report(predict_svm, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SGD (Stochastic Gradient Descent) Model\n",
    "sgd = Pipeline([('vect', CountVectorizer(tokenizer=tokenize,\n",
    "                                                  analyzer='word', ngram_range=(1, 2), min_df=1, lowercase=False)),\n",
    "                         ('tfidf', TfidfTransformer(sublinear_tf=True)),\n",
    "                         ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                                   alpha=1e-3, max_iter=5))])\n",
    "sgd = sgd.fit(X_train, y_train)\n",
    "sgd_score = sgd.score(X_test, y_test)\n",
    "print('SGD Model: ', sgd_score)\n",
    "predict_sgd = sgd.predict(X_test)\n",
    "print( \"f1 score : \", f1_score(predict_sgd, y_test))\n",
    "print(classification_report(predict_sgd, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "class_names = np.array([0, 1])\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.astype(int)\n",
    "predict_nb = predict_nb.astype(int)\n",
    "predict_svm = predict_svm.astype(int)\n",
    "predict_sgd = predict_sgd.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, predict_nb, classes=class_names)\n",
    "# plt.savefig('cm-nb.png')\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, predict_nb, classes=class_names, normalize=True)\n",
    "# plt.savefig('cm-nb-normalized.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, predict_svm, classes=class_names)\n",
    "# plt.savefig('cm-svm.png')\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, predict_svm, classes=class_names, normalize=True)\n",
    "# plt.savefig('cm-svm-normalized.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, predict_sgd, classes=class_names)\n",
    "# plt.savefig('cm-sgd.png')\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, predict_sgd, classes=class_names, normalize=True)\n",
    "# plt.savefig('cm-sgd-normalized.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## snapfood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/snappfood/train.csv', sep='\\t')\n",
    "dev = pd.read_csv('../data/snappfood/dev.csv', sep='\\t')\n",
    "test = pd.read_csv('../data/snappfood/test.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['processed_comment'] = train['comment'].map(preprocessing)\n",
    "dev['processed_comment'] = dev['comment'].map(preprocessing)\n",
    "test['processed_comment'] = test['comment'].map(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train['comment'], train['label_id']\n",
    "X_dev, y_dev = dev['comment'], dev['label_id']\n",
    "X_test, y_test = test['comment'], test['label_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Support Vector Machine Model\n",
    "svm = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize, analyzer='word', ngram_range=(1, 2), min_df=1, lowercase=False)),\n",
    "    ('tfidf', TfidfTransformer(sublinear_tf=True)),\n",
    "    ('clf-svm', LinearSVC(loss='hinge', penalty='l2', max_iter=5))\n",
    "                ])\n",
    "\n",
    "svm = svm.fit(X_train, y_train)\n",
    "linear_svc_score = svm.score(X_test, y_test)\n",
    "print('Linear SVC Model: ', linear_svc_score)\n",
    "predict_svm = svm.predict(X_dev)\n",
    "print( \"f1 score : \", f1_score(predict_svm, y_dev))\n",
    "print(classification_report(predict_svm, y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df[df['star']<5].iloc[2030:2050]\n",
    "predict_svm = svm.predict(test['description'])\n",
    "test['predicted'] = predict_svm\n",
    "test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
