{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from hazm import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "import re\n",
    "import emoji\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_path = \"../stopwords\"\n",
    "STOPWORDS = set([\n",
    "    \"از\", \"به\", \"در\", \"با\", \"که\", \"را\", \"تا\", \"و\", \"یا\", \"اما\", \"اگر\", \"برای\", \"بر\", \n",
    "    \"این\", \"آن\", \"یک\", \"هر\", \"هم\", \"همه\", \"چند\", \"چنین\", \"دیگر\", \"چون\", \"مثل\", \n",
    "    \"مانند\", \"چرا\", \"زیرا\", \"ولی\", \"آیا\", \"اگرچه\", \"لذا\", \"نیز\", \"باید\", \"می\", \n",
    "    \"باشد\", \"است\", \"بود\", \"هست\", \"شد\", \"شو\", \"باش\", \"کرد\", \"کن\", \"کند\", \"کرده\", \n",
    "    \"شده\", \"می‌شود\", \"خواهد\", \"خواهند\", \"خواهی\", \"خواهیم\", \"توان\", \"تواند\", \n",
    "    \"توانند\", \"توانست\", \"توانسته\", \"بوده\", \"نبود\", \"نباشد\", \"نیست\", \"نیستند\", \n",
    "    \"بودند\", \"باشند\", \"هستند\", \"دارم\", \"داری\", \"دارد\", \"دارند\", \"داریم\", \"داشت\", \n",
    "    \"داشتند\", \"داشته\", \"داشتم\", \"ندارم\", \"ندارد\", \"ندارند\", \"نداریم\", \"نداشت\", \n",
    "    \"نداشتند\", \"نداشته\", \"ای\", \"ایم\", \"اید\", \"اند\", \"ام\", \"ت\", \"ها\", \"های\", \"هایی\", \n",
    "    \"شان\", \"ش\", \"مان\", \"تان\", \"اینها\", \"آنها\", \"چیز\", \"چیزی\", \"چرا\", \"چه\", \"که\", \n",
    "    \"کدام\", \"چگونه\", \"چقدر\", \"چراکه\", \"آنان\", \"او\", \"آن\", \"ایشان\", \"ما\", \"شما\", \n",
    "    \"آنچه\", \"آنجا\", \"اینجا\", \"اینجاست\", \"آنجاست\", \"همان\", \"خود\", \"همه‌اش\", \n",
    "    \"هیچ\", \"هیچ‌کدام\", \"هرگز\", \"هیچگاه\", \"حالا\", \"اکنون\", \"دیروز\", \"امروز\", \n",
    "    \"فردا\", \"شب\", \"روز\", \"بعد\", \"قبل\", \"ساعت\", \"وقت\", \"زمان\", \"چندین\", \"بار\", \n",
    "    \"کم\", \"بیشتر\", \"کمتر\", \"حتی\", \"فقط\", \"تنها\", \"بالا\", \"پایین\", \"روی\", \"زیر\", \n",
    "    \"جلو\", \"پشت\", \"نزدیک\", \"دور\", \"وسط\", \"بیرون\", \"درون\", \"داخل\", \"کنار\", \n",
    "    \"اینجا\", \"آنجا\", \"هیچ‌جا\", \"هرجا\", \"هرکجا\", \"جا\", \"مکان\", \"محل\", \"چپ\", \"راست\", \n",
    "    \"بعدا\", \"سپس\", \"آنگاه\", \"دیگر\", \"چیزهای\", \"یعنی\", \"خب\", \"آره\", \"نه\", \"باشه\", \n",
    "    \"آها\", \"بله\", \"نمیدانم\", \"کسی\", \"دیگری\", \"هیچ‌کسی\", \"چیزها\"\n",
    "])\n",
    "# for filename in os.listdir(folder_path):\n",
    "#     file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "#     if os.path.isfile(file_path) and filename.endswith(\".txt\"):  \n",
    "#         with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "#             words = file.read().split()\n",
    "#             STOPWORDS.update(words)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('../data/BaSalam.reviews.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = reviews[(reviews['description'].notna())][['description', 'star']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_sticker(token):\n",
    "    # بررسی فرمت فایل\n",
    "    if re.match(r'.*\\.(webp|png|gif|jpg)$', token):\n",
    "        return True\n",
    "    # بررسی ایموجی\n",
    "    if emoji.is_emoji(token):\n",
    "        return True\n",
    "    # بررسی لینک\n",
    "    if re.match(r'https?://[^\\s]+', token):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "stemmer = Stemmer()\n",
    "\n",
    "\n",
    "def preprocessing(comment):\n",
    "    # حذف ایموجی‌ها\n",
    "    comment = emoji.replace_emoji(comment, replace=\"\")\n",
    "    # حذف لینک‌ها\n",
    "    comment = re.sub(r'https?://\\S+|www\\.\\S+', '', comment)\n",
    "    # حذف علامت‌های نگارشی\n",
    "    comment = re.sub(r'[^\\w\\s]', '', comment)\n",
    "    # حذف اعداد\n",
    "    comment = re.sub(r'\\d+', '', comment)\n",
    "    text =  comment\n",
    "    normalized = normalizer.normalize(text)\n",
    "    tokens = word_tokenize(normalized)\n",
    "    filtered = []\n",
    "    for token in tokens:\n",
    "        token = str(token)\n",
    "        token = token.lower()\n",
    "        token = re.sub(r'[\\u200c\\u200b\\u200d]', ' ', token)\n",
    "        if not token in STOPWORDS and not token.isdigit() and not is_sticker(token):\n",
    "            filtered.append(token)\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "# def preprocessing(text):\n",
    "#     punc_removed = text.translate(str.maketrans('', '', string.punctuation))\n",
    "#     normalized = normalizer.normalize(punc_removed)\n",
    "#     stemmed = stemmer.stem(normalized)\n",
    "#     tokens = word_tokenize(stemmed)\n",
    "#     filtered = []\n",
    "#     for token in tokens:\n",
    "#         token = str(token)\n",
    "#         token = token.lower()\n",
    "#         if not token in stopwords_list() and not token.isdigit():\n",
    "#             filtered.append(token)\n",
    "#     return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['satisfaction'] = df['star'].apply(lambda x: 1 if x > 3 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, to_much = train_test_split(df, test_size=0.95, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['cleaned_comment'] = df_train['description'].apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(df_train[df_train['cleaned_comment'].apply(lambda x: len(word_tokenize(x)) > 1)==False].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>همچی عین تو عکس دقیق اندازه ممنون</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>سلام یه کفش بی کیفیت لطفا خرید نکنید رایگان به...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>بسیار خوش عطر خورشت روغن انداخت لعاب داد کاملا...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>نظرم شبیه عکس تو عکس زیاد کیفیت ممنون غرفه دار...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>من خیلی کاربردی مخصوصا قهوه</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  label\n",
       "0                  همچی عین تو عکس دقیق اندازه ممنون      1\n",
       "1  سلام یه کفش بی کیفیت لطفا خرید نکنید رایگان به...      0\n",
       "2  بسیار خوش عطر خورشت روغن انداخت لعاب داد کاملا...      1\n",
       "3  نظرم شبیه عکس تو عکس زیاد کیفیت ممنون غرفه دار...      0\n",
       "4                        من خیلی کاربردی مخصوصا قهوه      1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df_train.reset_index()[['cleaned_comment', 'satisfaction']]\n",
    "data.columns = ['comment', 'label']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'] =data['label'].map({\n",
    "    0:'negative',\n",
    "    1:'positive'\n",
    "}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18470 entries, 0 to 18469\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   comment  18470 non-null  object\n",
      " 1   label    18470 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 288.7+ KB\n"
     ]
    }
   ],
   "source": [
    "negative_data = data[data['label'] == 'negative']\n",
    "positive_data = data[data['label'] == 'positive']\n",
    "\n",
    "cutting_point = min(len(negative_data), len(positive_data))\n",
    "\n",
    "if cutting_point <= len(negative_data):\n",
    "    negative_data = negative_data.sample(n=cutting_point).reset_index(drop=True)\n",
    "\n",
    "if cutting_point <= len(positive_data):\n",
    "    positive_data = positive_data.sample(n=cutting_point).reset_index(drop=True)\n",
    "\n",
    "new_data = pd.concat([negative_data, positive_data])\n",
    "new_data = new_data.sample(frac=1).reset_index(drop=True)\n",
    "new_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative', 'positive']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = list(sorted(data['label'].unique()))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14960, 3)\n",
      "(1663, 3)\n",
      "(1847, 3)\n"
     ]
    }
   ],
   "source": [
    "new_data['label_id'] = new_data['label'].apply(lambda t: labels.index(t))\n",
    "\n",
    "train, test = train_test_split(new_data, test_size=0.1, random_state=1, stratify=new_data['label'])\n",
    "train, valid = train_test_split(train, test_size=0.1, random_state=1, stratify=train['label'])\n",
    "\n",
    "train = train.reset_index(drop=True)\n",
    "valid = valid.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "x_train, y_train = train['comment'].values.tolist(), train['label_id'].values.tolist()\n",
    "x_valid, y_valid = valid['comment'].values.tolist(), valid['label_id'].values.tolist()\n",
    "x_test, y_test = test['comment'].values.tolist(), test['label_id'].values.tolist()\n",
    "\n",
    "print(train.shape)\n",
    "print(valid.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Courses&Code\\BaSalam-Project\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertTokenizer\n",
    "from transformers import BertModel\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n",
      "CUDA is not available.  Training on CPU ...\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'device: {device}')\n",
    "\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general config\n",
    "MAX_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 16\n",
    "TEST_BATCH_SIZE = 16\n",
    "\n",
    "EPOCHS = 3\n",
    "EEVERY_EPOCH = 1000\n",
    "LEARNING_RATE = 2e-5\n",
    "CLIP = 0.0\n",
    "\n",
    "MODEL_NAME_OR_PATH = 'HooshvareLab/bert-fa-base-uncased'\n",
    "OUTPUT_PATH = '/content/bert-fa-base-uncased-sentiment-taaghceh/pytorch_model.bin'\n",
    "\n",
    "os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label2id: {'negative': 0, 'positive': 1}\n",
      "id2label: {0: 'negative', 1: 'positive'}\n"
     ]
    }
   ],
   "source": [
    "# create a key finder based on label 2 id and id to label\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "print(f'label2id: {label2id}')\n",
    "print(f'id2label: {id2label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"positive\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.47.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 100000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# setup the tokenizer and configuration\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME_OR_PATH)\n",
    "config = BertConfig.from_pretrained(\n",
    "    MODEL_NAME_OR_PATH, **{\n",
    "        'label2id': label2id,\n",
    "        'id2label': id2label,\n",
    "    })\n",
    "\n",
    "print(config.to_json_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: \n",
      "فعلا بدستم رسیده استفاده کردم حتما نظر میدم ممنون ازغرفه دار\n",
      "negative\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(train))\n",
    "sample_comment = train.iloc[idx]['comment']\n",
    "sample_label = train.iloc[idx]['label']\n",
    "\n",
    "print(f'Sample: \\n{sample_comment}\\n{sample_label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Comment: فعلا بدستم رسیده استفاده کردم حتما نظر میدم ممنون ازغرفه دار\n",
      "   Tokens: فعلا بدستم رسیده استفاده کردم حتما نظر میدم ممنون ازغرفه دار\n",
      "Token IDs: [9771, 5529, 2015, 4583, 2988, 5501, 4567, 3138, 27409, 25303, 2791, 7969, 3395, 2916]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sample_comment)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(f'  Comment: {sample_comment}')\n",
    "print(f'   Tokens: {tokenizer.convert_tokens_to_string(tokens)}')\n",
    "print(f'Token IDs: {token_ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "\n",
      "input_ids:\n",
      "tensor([[    2,  9771,  5529,  2015,  4583,  2988,  5501,  4567,  3138, 27409,\n",
      "         25303,  2791,  7969,  3395,  2916,     4,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]])\n",
      "token_type_ids:\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "attention_mask:\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode_plus(\n",
    "    sample_comment,\n",
    "    max_length=32,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
    "    return_token_type_ids=True,\n",
    "    return_attention_mask=True,\n",
    "    padding='max_length',\n",
    "    return_tensors='pt',  # Return PyTorch tensors\n",
    ")\n",
    "\n",
    "print(f'Keys: {encoding.keys()}\\n')\n",
    "for k in encoding.keys():\n",
    "    print(f'{k}:\\n{encoding[k]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaaghcheDataset(torch.utils.data.Dataset):\n",
    "    \"\"\" Create a PyTorch dataset for Taaghche. \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, comments, targets=None, label_list=None, max_len=128):\n",
    "        self.comments = comments\n",
    "        self.targets = targets\n",
    "        self.has_target = isinstance(targets, list) or isinstance(targets, np.ndarray)\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "        \n",
    "        self.label_map = {label: i for i, label in enumerate(label_list)} if isinstance(label_list, list) else {}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.comments)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        comment = str(self.comments[item])\n",
    "\n",
    "        if self.has_target:\n",
    "            target = self.label_map.get(str(self.targets[item]), str(self.targets[item]))\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            comment,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt')\n",
    "        \n",
    "        inputs = {\n",
    "            'comment': comment,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "        }\n",
    "\n",
    "        if self.has_target:\n",
    "            inputs['targets'] = torch.tensor(target, dtype=torch.long)\n",
    "        \n",
    "        return inputs\n",
    "\n",
    "\n",
    "def create_data_loader(x, y, tokenizer, max_len, batch_size, label_list):\n",
    "    dataset = TaaghcheDataset(\n",
    "        comments=x,\n",
    "        targets=y,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len, \n",
    "        label_list=label_list)\n",
    "    \n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['negative', 'positive']\n",
    "train_data_loader = create_data_loader(train['comment'].to_numpy(), train['label'].to_numpy(), tokenizer, MAX_LEN, TRAIN_BATCH_SIZE, label_list)\n",
    "valid_data_loader = create_data_loader(valid['comment'].to_numpy(), valid['label'].to_numpy(), tokenizer, MAX_LEN, VALID_BATCH_SIZE, label_list)\n",
    "test_data_loader = create_data_loader(test['comment'].to_numpy(), None, tokenizer, MAX_LEN, TEST_BATCH_SIZE, label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['comment', 'input_ids', 'attention_mask', 'token_type_ids', 'targets'])\n",
      "['توجه قیمت خوب نظر کیفیت متوسط ارسال موقع', 'اصلا راضی نیستم مثلا سایز سفارش دادم اومده می بینم سایز کوچیک تره', 'همه اش سبز زرد داشتولی تمیز', 'ممنون وتشکر', 'بسیار بداونقدر خشک نمی شد کوبید بشه خورد من جاهای دیگه هلیله سیاه دیده بودماینطوری نبودن', 'سلام ممنون سپاسگزارم جناب آقای پریانی بابت ارسال سریع بسته بندی بسیار خوب کالای باکیفیت مرغوب آرزوی سلامتی خیر برکت برایتان', 'برام ارسال کردن آوردن آبکاری اش خوب مرجوع کردم یکبار دیگه برام ارسال کردن گلس هاش شکسته بودن آخر سر خودشون برام سه گلس خودشون برام آوردن دستشون درد نکنه آخر درست', 'باتشکر عوامل وغرفه داران باسلام سفارش رسید', 'تشکر غرفه دار محترم واقعا حلوا بسیار عالی ارسالشون سریع پیشنهاد می کنم یکبار حلوا استفاده کنید', 'سلام بسته من رسید قشنگه اونی تو عکس شلوارش کلا فرق میکنه', 'مزه شیره بد تمیز داخلش آشغال', 'کیفیت عالی', 'ممنون فروشنده بسته بندی وارسال خیلی خوب یه مزه آهن میده', 'کالا اشتباه فرستاده_شد', 'لباس راضیم برخورد فروشنده خوب', 'ارزانی بی حکمت']\n",
      "torch.Size([16, 128])\n",
      "tensor([   2, 3211, 3377, 4124, 3138, 4589, 5257, 4855, 6905,    4,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])\n",
      "torch.Size([16, 128])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "torch.Size([16, 128])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "torch.Size([16])\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "sample_data = next(iter(train_data_loader))\n",
    "\n",
    "print(sample_data.keys())\n",
    "\n",
    "print(sample_data['comment'])\n",
    "print(sample_data['input_ids'].shape)\n",
    "print(sample_data['input_ids'][0, :])\n",
    "print(sample_data['attention_mask'].shape)\n",
    "print(sample_data['attention_mask'][0, :])\n",
    "print(sample_data['token_type_ids'].shape)\n",
    "print(sample_data['token_type_ids'][0, :])\n",
    "print(sample_data['targets'].shape)\n",
    "print(sample_data['targets'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['comment', 'input_ids', 'attention_mask', 'token_type_ids'])\n"
     ]
    }
   ],
   "source": [
    "sample_test = next(iter(test_data_loader))\n",
    "print(sample_test.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentModel(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(SentimentModel, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(MODEL_NAME_OR_PATH)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        _, pooled_output = self.bert(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            token_type_ids=token_type_ids)\n",
    "        \n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA-SMI has failed because you do not have suffient permissions. Please try running as an administrator.\n"
     ]
    }
   ],
   "source": [
    "import torch, gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "pt_model = None\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Courses&Code\\BaSalam-Project\\env\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\alireza\\.cache\\huggingface\\hub\\models--HooshvareLab--bert-fa-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt_model <class '__main__.SentimentModel'>\n"
     ]
    }
   ],
   "source": [
    "pt_model = SentimentModel(config=config)\n",
    "pt_model = pt_model.to(device)\n",
    "\n",
    "print('pt_model', type(pt_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dropout(): argument 'input' (position 1) must be Tensor, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 20\u001b[0m\n\u001b[0;32m     13\u001b[0m sample_data_targets \u001b[38;5;241m=\u001b[39m sample_data_targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# outputs = F.softmax(\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#     pt_model(sample_data_input_ids, sample_data_attention_mask, sample_data_token_type_ids), \u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#     dim=1)\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpt_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_data_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_data_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_data_token_type_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m _, preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs[:\u001b[38;5;241m5\u001b[39m, :])\n",
      "File \u001b[1;32me:\\Courses&Code\\BaSalam-Project\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\Courses&Code\\BaSalam-Project\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[28], line 16\u001b[0m, in \u001b[0;36mSentimentModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask, token_type_ids):\n\u001b[0;32m     11\u001b[0m     _, pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert(\n\u001b[0;32m     12\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids, \n\u001b[0;32m     13\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, \n\u001b[0;32m     14\u001b[0m         token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids)\n\u001b[1;32m---> 16\u001b[0m     pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpooled_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(pooled_output)\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[1;32me:\\Courses&Code\\BaSalam-Project\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\Courses&Code\\BaSalam-Project\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32me:\\Courses&Code\\BaSalam-Project\\env\\lib\\site-packages\\torch\\nn\\modules\\dropout.py:70\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Courses&Code\\BaSalam-Project\\env\\lib\\site-packages\\torch\\nn\\functional.py:1425\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1423\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m-> 1425\u001b[0m     _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1426\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: dropout(): argument 'input' (position 1) must be Tensor, not str"
     ]
    }
   ],
   "source": [
    "# sample data output\n",
    "\n",
    "sample_data_comment = sample_data['comment']\n",
    "sample_data_input_ids = sample_data['input_ids']\n",
    "sample_data_attention_mask = sample_data['attention_mask']\n",
    "sample_data_token_type_ids = sample_data['token_type_ids']\n",
    "sample_data_targets = sample_data['targets']\n",
    "\n",
    "# available for using in GPU\n",
    "sample_data_input_ids = sample_data_input_ids.to(device)\n",
    "sample_data_attention_mask = sample_data_attention_mask.to(device)\n",
    "sample_data_token_type_ids = sample_data_token_type_ids.to(device)\n",
    "sample_data_targets = sample_data_targets.to(device)\n",
    "\n",
    "\n",
    "# outputs = F.softmax(\n",
    "#     pt_model(sample_data_input_ids, sample_data_attention_mask, sample_data_token_type_ids), \n",
    "#     dim=1)\n",
    "\n",
    "outputs = pt_model(sample_data_input_ids, sample_data_attention_mask, sample_data_token_type_ids)\n",
    "_, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "print(outputs[:5, :])\n",
    "print(preds[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_accuracy(y_true, y_pred):\n",
    "    return (y_true == y_pred).mean()\n",
    "\n",
    "def acc_and_f1(y_true, y_pred, average='weighted'):\n",
    "    acc = simple_accuracy(y_true, y_pred)\n",
    "    f1 = f1_score(y_true=y_true, y_pred=y_pred, average=average)\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "\n",
    "def y_loss(y_true, y_pred, losses):\n",
    "    y_true = torch.stack(y_true).cpu().detach().numpy()\n",
    "    y_pred = torch.stack(y_pred).cpu().detach().numpy()\n",
    "    y = [y_true, y_pred]\n",
    "    loss = np.mean(losses)\n",
    "\n",
    "    return y, loss\n",
    "\n",
    "\n",
    "def eval_op(model, data_loader, loss_fn):\n",
    "    model.eval()\n",
    "\n",
    "    losses = []\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for dl in tqdm(data_loader, total=len(data_loader), desc=\"Evaluation... \"):\n",
    "            \n",
    "            input_ids = dl['input_ids']\n",
    "            attention_mask = dl['attention_mask']\n",
    "            token_type_ids = dl['token_type_ids']\n",
    "            targets = dl['targets']\n",
    "\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            token_type_ids = token_type_ids.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # compute predicted outputs by passing inputs to the model\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids)\n",
    "            \n",
    "            # convert output probabilities to predicted class\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            # calculate the batch loss\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            # accumulate all the losses\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            y_pred.extend(preds)\n",
    "            y_true.extend(targets)\n",
    "    \n",
    "    eval_y, eval_loss = y_loss(y_true, y_pred, losses)\n",
    "    return eval_y, eval_loss\n",
    "\n",
    "\n",
    "def train_op(model, \n",
    "             data_loader, \n",
    "             loss_fn, \n",
    "             optimizer, \n",
    "             scheduler, \n",
    "             step=0, \n",
    "             print_every_step=100, \n",
    "             eval=False,\n",
    "             eval_cb=None,\n",
    "             eval_loss_min=np.Inf,\n",
    "             eval_data_loader=None, \n",
    "             clip=0.0):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    for dl in tqdm(data_loader, total=len(data_loader), desc=\"Training... \"):\n",
    "        step += 1\n",
    "\n",
    "        input_ids = dl['input_ids']\n",
    "        attention_mask = dl['attention_mask']\n",
    "        token_type_ids = dl['token_type_ids']\n",
    "        targets = dl['targets']\n",
    "\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # compute predicted outputs by passing inputs to the model\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids)\n",
    "        \n",
    "        # convert output probabilities to predicted class\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "        # calculate the batch loss\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        # accumulate all the losses\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        if clip > 0.0:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip)\n",
    "\n",
    "        # perform optimization step\n",
    "        optimizer.step()\n",
    "\n",
    "        # perform scheduler step\n",
    "        scheduler.step()\n",
    "\n",
    "        y_pred.extend(preds)\n",
    "        y_true.extend(targets)\n",
    "\n",
    "        if eval:\n",
    "            train_y, train_loss = y_loss(y_true, y_pred, losses)\n",
    "            train_score = acc_and_f1(train_y[0], train_y[1], average='weighted')\n",
    "\n",
    "            if step % print_every_step == 0:\n",
    "                eval_y, eval_loss = eval_op(model, eval_data_loader, loss_fn)\n",
    "                eval_score = acc_and_f1(eval_y[0], eval_y[1], average='weighted')\n",
    "\n",
    "                if hasattr(eval_cb, '__call__'):\n",
    "                    eval_loss_min = eval_cb(model, step, train_score, train_loss, eval_score, eval_loss, eval_loss_min)\n",
    "\n",
    "    train_y, train_loss = y_loss(y_true, y_pred, losses)\n",
    "\n",
    "    return train_y, train_loss, step, eval_loss_min"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
