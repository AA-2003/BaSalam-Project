{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "78thap7FSS0f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from tqdm import tqdm\n",
        "from hazm import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
        "import re\n",
        "import emoji\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from langdetect import detect, DetectorFactory\n",
        "import langid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6GgZPSCUpUz",
        "outputId": "e1fbe6b1-be9a-42cb-bd1e-268baac628ff"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('../data/BaSalam.reviews.csv', low_memory=True, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pP_BF5EEU6Gw",
        "outputId": "3fff25e5-42a6-438e-fa92-c5fc3c56fdab"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "STOPWORDS = set([\n",
        "    \"از\", \"به\", \"در\", \"با\", \"که\", \"را\", \"تا\", \"و\", \"یا\", \"اما\", \"اگر\", \"برای\", \"بر\",\n",
        "    \"این\", \"آن\", \"یک\", \"هر\", \"هم\", \"همه\", \"چند\", \"چنین\", \"دیگر\", \"چون\", \"مثل\",\n",
        "    \"مانند\", \"چرا\", \"زیرا\", \"ولی\", \"آیا\", \"اگرچه\", \"لذا\", \"نیز\", \"باید\", \"می\",\n",
        "    \"باشد\", \"است\", \"بود\", \"هست\", \"شد\", \"شو\", \"باش\", \"کرد\", \"کن\", \"کند\", \"کرده\",\n",
        "    \"شده\", \"می‌شود\", \"خواهد\", \"خواهند\", \"خواهی\", \"خواهیم\", \"توان\", \"تواند\",\n",
        "    \"توانند\", \"توانست\", \"توانسته\", \"بوده\", \"بودند\", \"باشند\", \"هستند\", \"دارم\", \"داری\", \"دارد\", \"دارند\", \"داریم\", \"داشت\",\n",
        "    \"داشتند\", \"داشته\", \"داشتم\", \"ای\", \"ایم\", \"اید\", \"اند\", \"ام\", \"ت\", \"ها\", \"های\", \"هایی\",\n",
        "    \"شان\", \"ش\", \"مان\", \"تان\", \"اینها\", \"آنها\", \"چیز\", \"چیزی\", \"چرا\", \"چه\", \"که\",\n",
        "    \"کدام\", \"چگونه\", \"چقدر\", \"چراکه\", \"آنان\", \"او\", \"آن\", \"ایشان\", \"ما\", \"شما\",\n",
        "    \"آنچه\", \"آنجا\", \"اینجا\", \"اینجاست\", \"آنجاست\", \"همان\", \"خود\", \"همه‌اش\",\n",
        "    \"هیچ\", \"هیچ‌کدام\", \"هرگز\", \"هیچگاه\", \"حالا\", \"اکنون\", \"دیروز\", \"امروز\",\n",
        "    \"فردا\", \"شب\", \"روز\", \"بعد\", \"قبل\", \"ساعت\", \"وقت\", \"زمان\", \"چندین\", \"بار\",\n",
        "    \"کم\", \"بیشتر\", \"کمتر\", \"حتی\", \"فقط\", \"تنها\", \"بالا\", \"پایین\", \"روی\", \"زیر\",\n",
        "    \"جلو\", \"پشت\", \"نزدیک\", \"دور\", \"وسط\", \"بیرون\", \"درون\", \"داخل\", \"کنار\",\n",
        "    \"اینجا\", \"آنجا\", \"هیچ‌جا\", \"هرجا\", \"هرکجا\", \"جا\", \"مکان\", \"محل\", \"چپ\", \"راست\",\n",
        "    \"بعدا\", \"سپس\", \"آنگاه\", \"دیگر\", \"چیزهای\", \"یعنی\", \"خب\", \"آره\", \"نه\", \"باشه\",\n",
        "    \"آها\", \"بله\", \"نمیدانم\", \"کسی\", \"دیگری\", \"هیچ‌کسی\", \"چیزها\"\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def is_sticker(token):\n",
        "    # بررسی فرمت فایل\n",
        "    if re.match(r'.*\\.(webp|png|gif|jpg)$', token):\n",
        "        return True\n",
        "    # بررسی ایموجی\n",
        "    if emoji.is_emoji(token):\n",
        "        return True\n",
        "    # بررسی لینک\n",
        "    if re.match(r'https?://[^\\s]+', token):\n",
        "        return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocessing(comment):\n",
        "    normalizer = Normalizer()\n",
        "    stemmer = Stemmer()\n",
        "    # حذف ایموجی‌ها\n",
        "    comment = emoji.replace_emoji(comment, replace=\" \")\n",
        "    # حذف لینک‌ها\n",
        "    comment = re.sub(r'https?://\\S+|www\\.\\S+', ' ', comment)\n",
        "    # حذف علامت‌های نگارشی\n",
        "    comment = re.sub(r'[^\\w\\s]', ' ', comment)\n",
        "    # حذف اعداد\n",
        "    comment = re.sub(r'\\d+', ' ', comment)\n",
        "    text = comment\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered = []\n",
        "    for token in tokens:\n",
        "        token = str(token)\n",
        "        token = token.lower()\n",
        "        token = re.sub(r'[\\u200c\\u200b\\u200d]', ' ', token)\n",
        "        if not token in STOPWORDS and not token.isdigit() and not is_sticker(token):\n",
        "            filtered.append(token)\n",
        "    return ' '.join(filtered)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize(text):\n",
        "    return word_tokenize(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detect_language(text):\n",
        "    lang, confidence = langid.classify(text)\n",
        "    return lang"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Clustering:\n",
        "    '''\n",
        "    This class is used to cluster the comments based on the stars.\n",
        "    It use different algorithms to cluster the comments and visualize the clusters in 3D.\n",
        "    '''\n",
        "    def __init__(self, df: pd.DataFrame, comments_col: str, star_col: str):\n",
        "        self.df = df\n",
        "        self.data = self.df[[comments_col, star_col]]\n",
        "        self.comments_col = comments_col\n",
        "        self.star_col = star_col\n",
        "        self.english_comments = None\n",
        "\n",
        "\n",
        "    def preprocess(self, ignore_eng: bool=True, sample:bool=False,\n",
        "                    balance_stars:bool=False, sample_size:int=10000, random_state:int=42, n_samples_per_star:int=1000):\n",
        "        \n",
        "        '''\n",
        "        This function is used to preprocess the comments and stars.\n",
        "        It can sample the comments and stars if the sample is True.\n",
        "        '''\n",
        "        self.data = self.data[(self.data[self.comments_col].notna()) & (self.data[self.star_col].notna())]\n",
        "\n",
        "        if sample:\n",
        "            if balance_stars:\n",
        "                self.data = self.data.groupby(self.star_col, group_keys=False).apply(\n",
        "                    lambda x: x.sample(n=min(n_samples_per_star, len(x)), random_state=random_state)\n",
        "                )\n",
        "            else:\n",
        "                self.data = self.data.sample(n=sample_size, random_state=random_state) \n",
        "                \n",
        "        self.data['len_comment_before_preprocessing'] = self.data[self.comments_col].apply(word_tokenize).map(len)\n",
        "        self.data = self.data[self.data['len_comment_before_preprocessing'] > 5]\n",
        "\n",
        "        if ignore_eng: \n",
        "            '''\n",
        "            ignore english comments\n",
        "            '''   \n",
        "            self.data['language'] = self.data[self.comments_col].apply(detect_language)\n",
        "            self.data = self.data[self.data['language'] == 'fa']\n",
        "            # self.data = self.data.reset_index(drop=True)\n",
        "        else:\n",
        "            pass\n",
        "            #  dont know how to handle english comments\n",
        "\n",
        "        \n",
        "        #  plot the distribution of comments based on stars\n",
        "        fig = go.Figure()\n",
        "\n",
        "        groupby_rate = self.data.groupby(self.star_col)[self.star_col].count()\n",
        "\n",
        "        fig.add_trace(go.Bar(\n",
        "            x=list(sorted(groupby_rate.index)),\n",
        "            y=groupby_rate.tolist(),\n",
        "            text=groupby_rate.tolist(),\n",
        "            textposition='auto'\n",
        "        ))\n",
        "\n",
        "        fig.update_layout(\n",
        "            title_text='Distribution of star within comments',\n",
        "            xaxis_title_text='Rate',\n",
        "            yaxis_title_text='Frequency',\n",
        "            bargap=0.2,\n",
        "            bargroupgap=0.2)\n",
        "\n",
        "        fig.show()\n",
        "        \n",
        "        # self.data = self.data.reset_index(drop=True)\n",
        "        self.data['cleaned_comment'] = self.data[self.comments_col].apply(preprocessing)\n",
        "        self.data['len_comment_after_preprocessing'] = self.data['cleaned_comment'].apply(word_tokenize).map(len)\n",
        "\n",
        "    def vectorize(self):\n",
        "        ''' \n",
        "        This function is used to vectorize the comments.\n",
        "        '''\n",
        "        self.vectorizer_pipeline = Pipeline([\n",
        "            ('vect', CountVectorizer(tokenizer=tokenize, analyzer='word', ngram_range=(1, 3), min_df=1, lowercase=False)),\n",
        "            ('tfidf', TfidfTransformer(sublinear_tf=True))\n",
        "            ])\n",
        "        self.vectorized_comments = self.vectorizer_pipeline.fit_transform(self.data['cleaned_comment'])\n",
        "    \n",
        "    def kmeans_clustering(self, elbow:bool=False, n_clusters:int=4):\n",
        "        '''\n",
        "        This function is used to cluster the comments using KMeans.\n",
        "        It can use elbow method to find the optimal number of clusters.\n",
        "        '''\n",
        "        if elbow:\n",
        "            # Elbow for finding number of clusters\n",
        "            cluster_range = range(2, 7)\n",
        "            wcss = []\n",
        "\n",
        "            for n_clusters in cluster_range:\n",
        "                self.kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "                self.kmeans.fit(self.vectorized_comments)\n",
        "\n",
        "                wcss.append(self.kmeans.inertia_)\n",
        "\n",
        "            self.data['cluster_kmeans'] = self.kmeans.predict(self.vectorized_comments)\n",
        "            # Plot the Elbow\n",
        "            plt.figure(figsize=(8, 5))\n",
        "            plt.plot(cluster_range, wcss, marker='o', linestyle='-', color='b')\n",
        "            plt.title('Elbow Method for Optimal Number of Clusters')\n",
        "            plt.xlabel('Number of Clusters')\n",
        "            plt.ylabel('Within-Cluster-Sum of Squared Errors (WCSS)')\n",
        "            plt.xticks(cluster_range)\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "\n",
        "        else:\n",
        "            self.kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "            self.kmeans.fit(self.vectorized_comments)\n",
        "            self.data['cluster_kmeans'] = self.kmeans.predict(self.vectorized_comments)\n",
        "    \n",
        "    def dbscan_clustering(self):\n",
        "        '''\n",
        "        This function is used to cluster the comments using DBSCAN.\n",
        "        '''\n",
        "        dbscan = DBSCAN(eps=0.5, min_samples=50, metric='cosine')\n",
        "        clusters = dbscan.fit_predict(self.vectorized_comments)\n",
        "\n",
        "        self.data['cluster_dbscan'] = clusters\n",
        "            \n",
        "    def plot_3d(self, hover_data:list=None, color:str='cluster_kmeans', pca:bool=False):\n",
        "        '''\n",
        "        This function is used to plot the clusters in 3D.\n",
        "        color: [star, cluster_kmeans, cluster_dbscan] \n",
        "        '''\n",
        "        # Reduce TF-IDF vectors to 3 dimensions\n",
        "        if pca:\n",
        "            pca = PCA(n_components=3)\n",
        "            reduced_data = pca.fit_transform(self.vectorized_comments)\n",
        "        else: \n",
        "            svd = TruncatedSVD(n_components=3)\n",
        "            reduced_data = svd.fit_transform(self.vectorized_comments)\n",
        "\n",
        "        self.data['x'] = reduced_data[:, 0]\n",
        "        self.data['y'] = reduced_data[:, 1]\n",
        "        self.data['z'] = reduced_data[:, 2]\n",
        "\n",
        "        #  3D scatter plot\n",
        "        fig = px.scatter_3d(self.data, x='x', y='y', z='z', color=color,\n",
        "                            labels={'x': 'Component 1', 'y': 'Component 2', 'z': 'Component 3'},\n",
        "                            title=f'3D Visualization on {color}',\n",
        "                            hover_data=['cleaned_comment', 'star'])\n",
        "        fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "c = Clustering(df, 'description', 'star')\n",
        "c.preprocess(sample=True, balance_stars=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "c.vectorize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "c.kmeans_clustering(elbow=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "c.kmeans_clustering(n_clusters=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "c.dbscan_clustering()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "c.plot_3d()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "c.plot_3d(color='cluster_dbscan', pca=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "c.plot_3d(color='star')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "c.data[['x', 'y', 'z', 'star']].corr()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
